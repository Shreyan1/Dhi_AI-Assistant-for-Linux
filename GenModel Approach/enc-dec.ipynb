{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-22 18:01:03.829532: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1732278663.846761  119688 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1732278663.852115  119688 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-22 18:01:03.868960: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from librarymaster import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('../materials/paired_nl_bash.csv',delimiter=',')\n",
    "# df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = df['natural_language']\n",
    "# input_texts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_texts = df['bash_command']\n",
    "# output_texts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIAL_TOKENS = ['<start>', '<end>', '<unk>', '<pad>']\n",
    "#Tokenise and pad sequences\n",
    "# input_tokeniser = Tokenizer()\n",
    "# output_tokeniser = Tokenizer()\n",
    "input_tokeniser = Tokenizer(oov_token='<unk>')  # Adding out-of-vocabulary token\n",
    "output_tokeniser = Tokenizer(oov_token='<unk>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokeniser.fit_on_texts(input_texts)\n",
    "# output_tokeniser.fit_on_texts(output_texts)\n",
    "output_texts_with_tokens = ['<start> ' + text + ' <end>' for text in output_texts]\n",
    "output_tokeniser.fit_on_texts(output_texts_with_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Special tokens not in vocabulary. Adding them...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'SPECIAL_TOKENS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Add special tokens to vocabulary if they're missing\u001b[39;00m\n\u001b[1;32m      4\u001b[0m current_vocab_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(output_tokeniser\u001b[38;5;241m.\u001b[39mword_index)\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mSPECIAL_TOKENS\u001b[49m, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m output_tokeniser\u001b[38;5;241m.\u001b[39mword_index:\n\u001b[1;32m      7\u001b[0m         output_tokeniser\u001b[38;5;241m.\u001b[39mword_index[token] \u001b[38;5;241m=\u001b[39m current_vocab_size \u001b[38;5;241m+\u001b[39m i\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SPECIAL_TOKENS' is not defined"
     ]
    }
   ],
   "source": [
    "if '<start>' not in output_tokeniser.word_index:\n",
    "    print(\"Warning: Special tokens not in vocabulary. Adding them...\")\n",
    "    # Add special tokens to vocabulary if they're missing\n",
    "    current_vocab_size = len(output_tokeniser.word_index)\n",
    "    for i, token in enumerate(SPECIAL_TOKENS, start=1):\n",
    "        if token not in output_tokeniser.word_index:\n",
    "            output_tokeniser.word_index[token] = current_vocab_size + i\n",
    "            output_tokeniser.index_word[current_vocab_size + i] = token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # input_vocabsize, output_vocabsize\n",
    "input_vocabsize = len(input_tokeniser.word_index) + 1\n",
    "output_vocabsize = len(output_tokeniser.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sequence processing\n",
    "input_sequences = pad_sequences(input_tokeniser.texts_to_sequences(input_texts), \n",
    "                              padding='post')\n",
    "output_sequences = pad_sequences(output_tokeniser.texts_to_sequences(output_texts_with_tokens),\n",
    "                               padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "embedding_dim=128\n",
    "units=256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Encoder\n",
    "# encoder_inputs = tf.keras.Input(shape=(None,))\n",
    "# encoder_embedding = Embedding(input_vocabsize, embedding_dim)(encoder_inputs)\n",
    "# encoder_lstm = LSTM(units, return_state=True)\n",
    "# encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Decoder with attention mechanism\n",
    "# decoder_inputs = tf.keras.Input(shape=(None,))\n",
    "# decoder_embedding = Embedding(output_vocabsize, embedding_dim)(decoder_inputs)\n",
    "# decoder_lstm = LSTM(units, return_sequences=True, return_state=True)\n",
    "# decoder_outputs, _, _ = decoder_lstm(decoder_embedding, \n",
    "#                                    initial_state=[state_h, state_c])\n",
    "# decoder_dense = Dense(output_vocabsize, activation='softmax')\n",
    "# output = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training Model\n",
    "# model = Model([encoder_inputs, decoder_inputs], output)\n",
    "# model.compile(optimizer='adam', \n",
    "#              loss='sparse_categorical_crossentropy',\n",
    "#              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_model():\n",
    "    # Encoder\n",
    "    encoder_inputs = tf.keras.Input(shape=(None,))\n",
    "    encoder_embedding = Embedding(input_vocabsize, embedding_dim, name='embedding')(encoder_inputs)\n",
    "    encoder_lstm = LSTM(units, return_state=True, name='lstm')\n",
    "    encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "\n",
    "    # Decoder\n",
    "    decoder_inputs = tf.keras.Input(shape=(None,))\n",
    "    decoder_embedding = Embedding(output_vocabsize, embedding_dim, name='embedding_1')(decoder_inputs)\n",
    "    decoder_lstm = LSTM(units, return_sequences=True, return_state=True, name='lstm_1')\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=[state_h, state_c])\n",
    "    decoder_dense = Dense(output_vocabsize, activation='softmax', name='dense')\n",
    "    output = decoder_dense(decoder_outputs)\n",
    "\n",
    "    # Create and compile model\n",
    "    model = Model([encoder_inputs, decoder_inputs], output)\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inference Setup\n",
    "def create_inference_models(trained_model):\n",
    "    # Get the layers from trained model\n",
    "    encoder_inputs = trained_model.input[0]\n",
    "    encoder_embedding = trained_model.get_layer('embedding')\n",
    "    encoder_lstm = trained_model.get_layer('lstm')\n",
    "    \n",
    "    # Recreate encoder model\n",
    "    encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding(encoder_inputs))\n",
    "    encoder_model = Model(encoder_inputs, [state_h, state_c])\n",
    "    \n",
    "    # Decoder setup\n",
    "    decoder_inputs = Input(shape=(None,))\n",
    "    decoder_embedding = trained_model.get_layer('embedding_1')\n",
    "    decoder_lstm = trained_model.get_layer('lstm_1')\n",
    "    decoder_dense = trained_model.get_layer('dense')\n",
    "    \n",
    "    decoder_state_input_h = Input(shape=(units,))\n",
    "    decoder_state_input_c = Input(shape=(units,))\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    \n",
    "    decoder_outputs = decoder_embedding(decoder_inputs)\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "        decoder_outputs, initial_state=decoder_states_inputs\n",
    "    )\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    \n",
    "    decoder_model = Model(\n",
    "        [decoder_inputs] + decoder_states_inputs,\n",
    "        [decoder_outputs, state_h, state_c]\n",
    "    )\n",
    "    \n",
    "    return encoder_model, decoder_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_command(input_query, max_length=50):\n",
    "    # Input preprocessing\n",
    "    input_seq = input_tokeniser.texts_to_sequences([input_query])\n",
    "    input_seq = pad_sequences(input_seq, maxlen=input_sequences.shape[1], padding='post')\n",
    "    \n",
    "    # Get initial states from encoder\n",
    "    states_value = encoder_model.predict(input_seq, verbose=0)\n",
    "    \n",
    "    # Initialize target sequence with start token\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    start_token_idx = output_tokeniser.word_index.get('<start>', 1)  # Default to 1 if not found\n",
    "    target_seq[0, 0] = start_token_idx\n",
    "    \n",
    "    decoded_sentence = []\n",
    "    \n",
    "    while True:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value, verbose=0)\n",
    "        \n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_word = output_tokeniser.index_word.get(sampled_token_index, '<unk>')\n",
    "        \n",
    "        if sampled_word == '<end>' or len(decoded_sentence) > max_length:\n",
    "            break\n",
    "            \n",
    "        if sampled_word not in SPECIAL_TOKENS:  # Skip all special tokens\n",
    "            decoded_sentence.append(sampled_word)\n",
    "        \n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "        states_value = [h, c]\n",
    "    \n",
    "    return ' '.join(decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enable this by uncommenting only when you need it to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-22 18:01:09.220768: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 622ms/step - accuracy: 0.8600 - loss: 2.4384 - val_accuracy: 0.9029 - val_loss: 0.6260\n",
      "Epoch 2/20\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 630ms/step - accuracy: 0.8948 - loss: 0.6691 - val_accuracy: 0.9063 - val_loss: 0.6003\n",
      "Epoch 3/20\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 647ms/step - accuracy: 0.8986 - loss: 0.6392 - val_accuracy: 0.9091 - val_loss: 0.5774\n",
      "Epoch 4/20\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 671ms/step - accuracy: 0.9000 - loss: 0.6110 - val_accuracy: 0.9098 - val_loss: 0.5581\n",
      "Epoch 5/20\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 668ms/step - accuracy: 0.9015 - loss: 0.5878 - val_accuracy: 0.9114 - val_loss: 0.5418\n",
      "Epoch 6/20\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 672ms/step - accuracy: 0.9027 - loss: 0.5710 - val_accuracy: 0.9119 - val_loss: 0.5307\n",
      "Epoch 7/20\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 669ms/step - accuracy: 0.9053 - loss: 0.5450 - val_accuracy: 0.9145 - val_loss: 0.5176\n",
      "Epoch 8/20\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 665ms/step - accuracy: 0.9081 - loss: 0.5209 - val_accuracy: 0.9151 - val_loss: 0.5071\n",
      "Epoch 9/20\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 648ms/step - accuracy: 0.9095 - loss: 0.5060 - val_accuracy: 0.9162 - val_loss: 0.4996\n",
      "Epoch 10/20\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 663ms/step - accuracy: 0.9105 - loss: 0.4887 - val_accuracy: 0.9166 - val_loss: 0.4951\n",
      "Epoch 11/20\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 659ms/step - accuracy: 0.9112 - loss: 0.4782 - val_accuracy: 0.9179 - val_loss: 0.4877\n",
      "Epoch 12/20\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 663ms/step - accuracy: 0.9133 - loss: 0.4610 - val_accuracy: 0.9184 - val_loss: 0.4845\n",
      "Epoch 13/20\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 662ms/step - accuracy: 0.9145 - loss: 0.4441 - val_accuracy: 0.9178 - val_loss: 0.4820\n",
      "Epoch 14/20\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 661ms/step - accuracy: 0.9165 - loss: 0.4294 - val_accuracy: 0.9185 - val_loss: 0.4789\n",
      "Epoch 15/20\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 663ms/step - accuracy: 0.9174 - loss: 0.4187 - val_accuracy: 0.9192 - val_loss: 0.4760\n",
      "Epoch 16/20\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 662ms/step - accuracy: 0.9184 - loss: 0.4061 - val_accuracy: 0.9187 - val_loss: 0.4747\n",
      "Epoch 17/20\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 668ms/step - accuracy: 0.9194 - loss: 0.3953 - val_accuracy: 0.9191 - val_loss: 0.4730\n",
      "Epoch 18/20\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 670ms/step - accuracy: 0.9199 - loss: 0.3872 - val_accuracy: 0.9197 - val_loss: 0.4712\n",
      "Epoch 19/20\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 649ms/step - accuracy: 0.9205 - loss: 0.3814 - val_accuracy: 0.9201 - val_loss: 0.4699\n",
      "Epoch 20/20\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 659ms/step - accuracy: 0.9225 - loss: 0.3681 - val_accuracy: 0.9203 - val_loss: 0.4681\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x75a757902ba0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create and train the model\n",
    "model = create_training_model()\n",
    "model.fit(\n",
    "    [input_sequences, output_sequences[:, :-1]],\n",
    "    output_sequences[:, 1:],\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create inference models\n",
    "encoder_model, decoder_model = create_inference_models(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'<start>'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Test the model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m input_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(BSD specific) Display process information twice, waiting one second between each, filtering out the header line.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m generated_command \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_query\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated Command: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgenerated_command\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[15], line 11\u001b[0m, in \u001b[0;36mgenerate_command\u001b[0;34m(input_query, max_length)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Initialize target sequence\u001b[39;00m\n\u001b[1;32m     10\u001b[0m target_seq \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 11\u001b[0m target_seq[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43moutput_tokeniser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_index\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m<start>\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     13\u001b[0m decoded_sentence \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: '<start>'"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "input_query = \"(BSD specific) Display process information twice, waiting one second between each, filtering out the header line.\"\n",
    "generated_command = generate_command(input_query)\n",
    "print(f\"Generated Command: {generated_command}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Train the model\n",
    "# model.fit(\n",
    "#     [input_sequences, output_sequences[:, :-1]],\n",
    "#     output_sequences[:, 1:],\n",
    "#     batch_size=64,\n",
    "#     epochs=20,\n",
    "#     validation_split=0.2\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_model_and_tokenizers(model, input_tokeniser, output_tokeniser, 'command_generator')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_training_model():\n",
    "#     # Encoder\n",
    "#     encoder_inputs = tf.keras.Input(shape=(None,))\n",
    "#     encoder_embedding = Embedding(input_vocabsize, embedding_dim, name='embedding')(encoder_inputs)\n",
    "#     encoder_lstm = LSTM(units, return_state=True, name='lstm')\n",
    "#     encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "\n",
    "#     # Decoder\n",
    "#     decoder_inputs = tf.keras.Input(shape=(None,))\n",
    "#     decoder_embedding = Embedding(output_vocabsize, embedding_dim, name='embedding_1')(decoder_inputs)\n",
    "#     decoder_lstm = LSTM(units, return_sequences=True, return_state=True, name='lstm_1')\n",
    "#     decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=[state_h, state_c])\n",
    "#     decoder_dense = Dense(output_vocabsize, activation='softmax', name='dense')\n",
    "#     output = decoder_dense(decoder_outputs)\n",
    "\n",
    "#     # Create and compile model\n",
    "#     model = Model([encoder_inputs, decoder_inputs], output)\n",
    "#     model.compile(optimizer='adam', \n",
    "#                  loss='sparse_categorical_crossentropy',\n",
    "#                  metrics=['accuracy'])\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create inference models\n",
    "# encoder_model, decoder_model = create_inference_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate commands\n",
    "# input_query = \"(BSD specific) Display process information twice, waiting one second between each, filtering out the header line.\"\n",
    "# generated_command = generate_command(input_query)\n",
    "# print(f\"Generated Command: {generated_command}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def decode_sequence(input_text):\n",
    "#     #Tokenize input\n",
    "#     input_seq = pad_sequences(input_tokeniser.texts_to_sequences([input_texts]),\n",
    "#                               maxlen=input_sequences.shape[1])\n",
    "#     states_value=encoder_model.predict(input_seq)\n",
    "\n",
    "#     #Start token for ecoder\n",
    "#     target_seq=np.zeros((1,1))\n",
    "#     target_seq[0,0]=output_tokeniser.word_index['<start>']\n",
    "\n",
    "#     #Loop to generate command\n",
    "#     generated_command=''\n",
    "#     for _ in range(max_length):\n",
    "#         output_tokens,h,c=decoder_model.predict([target_seq]+states_value)\n",
    "#         sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "#         sampled_word = output_tokeniser.index_word[sampled_token_index]\n",
    "\n",
    "#         if sampled_word == '<end>':\n",
    "#             break\n",
    "\n",
    "#         generated_command += ' ' + sampled_word\n",
    "\n",
    "#         target_seq = np.zeros((1, 1))\n",
    "#         target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "#         states_value = [h, c]\n",
    "\n",
    "#     return generated_command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #saving the model\n",
    "# model.save('genCommand.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Saving the input tokenizer\n",
    "# with open('input_tokeniser.pkl', 'wb') as handle:\n",
    "#     pickle.dump(input_tokeniser, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# # Saving the output tokenizer\n",
    "# with open('output_tokeniser.pkl', 'wb') as handle:\n",
    "#     pickle.dump(output_tokeniser, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = load_model('genCommand.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Encoder setup\n",
    "# encoder_inputs = model.input[0]  # Input for the encoder\n",
    "# encoder_outputs, state_h, state_c = model.layers[4].output  # Outputs from the encoder LSTM\n",
    "# encoder_model = Model(encoder_inputs, [state_h, state_c])  # Define the encoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define decoder input sequence\n",
    "# decoder_inputs = Input(shape=(None,))  # Placeholder for decoder input sequences\n",
    "# decoder_embedding = model.layers[3](decoder_inputs)  # Embedding layer for decoder\n",
    "\n",
    "# # Retrieve LSTM units\n",
    "# units = model.layers[5].units\n",
    "\n",
    "# # Placeholders for decoder LSTM states\n",
    "# decoder_state_input_h = Input(shape=(units,))\n",
    "# decoder_state_input_c = Input(shape=(units,))\n",
    "# decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# # Recreate the decoder LSTM layer for inference\n",
    "# decoder_lstm_inference = LSTM(\n",
    "#     units,\n",
    "#     return_sequences=True,\n",
    "#     return_state=True,\n",
    "#     name=\"decoder_lstm_inference\",\n",
    "# )\n",
    "# decoder_outputs, state_h, state_c = decoder_lstm_inference(\n",
    "#     decoder_embedding, initial_state=decoder_states_inputs\n",
    "# )\n",
    "\n",
    "# # Dense layer for output\n",
    "# decoder_dense = model.layers[6]\n",
    "# decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# # Define the decoder model\n",
    "# decoder_model = Model(\n",
    "#     [decoder_inputs] + decoder_states_inputs,  # Inputs for decoder sequence and states\n",
    "#     [decoder_outputs, state_h, state_c],  # Outputs: predictions and new states\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_tokeniser.word_index['<start>'] = len(output_tokeniser.word_index) + 1\n",
    "# output_tokeniser.word_index['<end>'] = len(output_tokeniser.word_index) + 1\n",
    "# output_tokeniser.index_word[output_tokeniser.word_index['<start>']] = '<start>'\n",
    "# output_tokeniser.index_word[output_tokeniser.word_index['<end>']] = '<end>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('<start>' in output_tokeniser.word_index)  # Should return True\n",
    "# print('<end>' in output_tokeniser.word_index)    # Should return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def generate_command(input_text, max_length=50):\n",
    "#     # Tokenize and pad the input\n",
    "#     input_seq = pad_sequences(\n",
    "#         input_tokeniser.texts_to_sequences([input_text]),\n",
    "#         maxlen=input_sequences.shape[1],  # Ensure padding matches the trained input size\n",
    "#         padding='post'\n",
    "#     )\n",
    "\n",
    "#     # Encode the input\n",
    "#     states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "#     # Start the decoding process\n",
    "#     # Start the decoding process\n",
    "#     start_token_id = output_tokeniser.word_index['<start>']\n",
    "#     target_seq = pad_sequences([[start_token_id]], maxlen=1)\n",
    "#     decoded_sentence = ''\n",
    "\n",
    "    \n",
    "#     for _ in range(max_length):\n",
    "#         # Predict the next token\n",
    "#         output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "        \n",
    "#         # Get the most likely token\n",
    "#         sampled_token_index = output_tokens.argmax(axis=-1)[0, -1]\n",
    "#         sampled_word = output_tokeniser.index_word[sampled_token_index]\n",
    "        \n",
    "#         # Break if the end token is reached\n",
    "#         if sampled_word == '<end>':\n",
    "#             break\n",
    "\n",
    "#         decoded_sentence += ' ' + sampled_word\n",
    "        \n",
    "#         # Update the target sequence\n",
    "#         target_seq = pad_sequences([[sampled_token_index]], maxlen=1, padding='post')\n",
    "\n",
    "#         # Update states\n",
    "#         states_value = [h, c]\n",
    "    \n",
    "#     return decoded_sentence.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(output_tokeniser.word_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # If the embedding layer has a name\n",
    "# embedding_layer = model.get_layer('embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Embedding vocabulary size: {embedding_layer.input_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check the embedding layer's vocabulary size\n",
    "# print(f\"Embedding vocabulary size: {embedding_layer.input_dim}\")\n",
    "\n",
    "# # Example: Tokenize the input query\n",
    "# input_query = \"(BSD specific) Display process information twice, waiting one second between each, filtering out the header line.\"\n",
    "# tokenized_input = input_tokeniser.texts_to_sequences([input_query])  # Ensure you use the trained tokenizer\n",
    "# print(f\"Max token index in input: {max(tokenized_input[0])}\")\n",
    "\n",
    "# # Validate input tokens\n",
    "# if max(tokenized_input[0]) >= embedding_layer.input_dim:\n",
    "#     print(\"Error: Input token exceeds embedding layer vocabulary size.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_max_seq_length = encoder_model.input_shape[1]\n",
    "# # encoder_max_seq_length,  target_tokeniser.word_index['<start>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_command(input_query, max_length=50):\n",
    "#     # Add input validation\n",
    "#     tokenized_input = input_tokeniser.texts_to_sequences([input_query])\n",
    "#     if not tokenized_input[0]:  # Check if tokenization produced empty sequence\n",
    "#         return \"Error: Could not tokenize input\"\n",
    "    \n",
    "#     # Ensure all token indices are within bounds\n",
    "#     if max(tokenized_input[0]) >= input_vocabsize:\n",
    "#         return \"Error: Input contains unknown tokens\"\n",
    "        \n",
    "#     tokenized_input = pad_sequences(tokenized_input, \n",
    "#                                   maxlen=encoder_max_seq_length, \n",
    "#                                   padding='post')\n",
    "    \n",
    "#     # Get encoder states\n",
    "#     states_value = encoder_model.predict(tokenized_input, verbose=0)\n",
    "    \n",
    "#     # Initialize with start token\n",
    "#     target_seq = np.zeros((1, 1))\n",
    "#     start_token_idx = output_tokeniser.word_index.get('<start>', 1)  # Default to 1 if not found\n",
    "#     target_seq[0, 0] = start_token_idx\n",
    "    \n",
    "#     decoded_sentence = []\n",
    "    \n",
    "#     for _ in range(max_length):\n",
    "#         output_tokens, h, c = decoder_model.predict([target_seq] + states_value, verbose=0)\n",
    "        \n",
    "#         sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "#         sampled_word = output_tokeniser.index_word.get(sampled_token_index, '<unk>')\n",
    "        \n",
    "#         if sampled_word == '<end>':\n",
    "#             break\n",
    "            \n",
    "#         if sampled_word != '<unk>':\n",
    "#             decoded_sentence.append(sampled_word)\n",
    "            \n",
    "#         target_seq = np.zeros((1, 1))\n",
    "#         target_seq[0, 0] = sampled_token_index\n",
    "#         states_value = [h, c]\n",
    "    \n",
    "#     return ' '.join(decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_query = \"(BSD specific) Display process information twice, waiting one second between each, filtering out the header line.\"\n",
    "# generated_command = generate_command(input_query)\n",
    "# print(f\"Generated Command: {generated_command}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dhienv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

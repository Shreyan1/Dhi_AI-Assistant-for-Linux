{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-26 15:31:49.160745: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1732615309.177665   73164 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1732615309.182322   73164 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-26 15:31:49.199318: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from masterlibrary import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('../materials/paired_nl_bash.csv', delimiter=',')\n",
    "input_texts = df['natural_language']\n",
    "output_texts = df['bash_command']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Special tokens not in vocabulary. Adding them...\n"
     ]
    }
   ],
   "source": [
    "# Define special tokens\n",
    "SPECIAL_TOKENS = ['<start>', '<end>', '<unk>', '<pad>']\n",
    "\n",
    "# Initialize tokenizers\n",
    "input_tokeniser = Tokenizer(oov_token='<unk>')\n",
    "output_tokeniser = Tokenizer(oov_token='<unk>')\n",
    "\n",
    "# Prepare texts\n",
    "input_tokeniser.fit_on_texts(input_texts)\n",
    "output_texts_with_tokens = ['<start> ' + text + ' <end>' for text in output_texts]\n",
    "output_tokeniser.fit_on_texts(output_texts_with_tokens)\n",
    "\n",
    "# Add special tokens if missing\n",
    "if '<start>' not in output_tokeniser.word_index:\n",
    "    print(\"Warning: Special tokens not in vocabulary. Adding them...\")\n",
    "    current_vocab_size = len(output_tokeniser.word_index)\n",
    "    for i, token in enumerate(SPECIAL_TOKENS, start=1):\n",
    "        if token not in output_tokeniser.word_index:\n",
    "            output_tokeniser.word_index[token] = current_vocab_size + i\n",
    "            output_tokeniser.index_word[current_vocab_size + i] = token\n",
    "\n",
    "# Create sequences\n",
    "input_sequences = pad_sequences(input_tokeniser.texts_to_sequences(input_texts), \n",
    "                              padding='post')\n",
    "output_sequences = pad_sequences(output_tokeniser.texts_to_sequences(output_texts_with_tokens),\n",
    "                               padding='post')\n",
    "\n",
    "# Define vocabulary sizes\n",
    "input_vocabsize = len(input_tokeniser.word_index) + 1\n",
    "output_vocabsize = len(output_tokeniser.word_index) + 1\n",
    "\n",
    "# Model parameters\n",
    "embedding_dim = 128\n",
    "units = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_model():\n",
    "    # Encoder\n",
    "    encoder_inputs = tf.keras.Input(shape=(None,))\n",
    "    encoder_embedding = Embedding(input_vocabsize, embedding_dim, name='embedding')(encoder_inputs)\n",
    "    encoder_lstm = LSTM(units, return_state=True, name='lstm')\n",
    "    encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "\n",
    "    # Decoder\n",
    "    decoder_inputs = tf.keras.Input(shape=(None,))\n",
    "    decoder_embedding = Embedding(output_vocabsize, embedding_dim, name='embedding_1')(decoder_inputs)\n",
    "    decoder_lstm = LSTM(units, return_sequences=True, return_state=True, name='lstm_1')\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=[state_h, state_c])\n",
    "    decoder_dense = Dense(output_vocabsize, activation='softmax', name='dense')\n",
    "    output = decoder_dense(decoder_outputs)\n",
    "\n",
    "    # Create and compile model\n",
    "    model = Model([encoder_inputs, decoder_inputs], output)\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inference_models(trained_model):\n",
    "    # Get the layers from trained model\n",
    "    encoder_inputs = trained_model.input[0]\n",
    "    encoder_embedding = trained_model.get_layer('embedding')\n",
    "    encoder_lstm = trained_model.get_layer('lstm')\n",
    "    \n",
    "    # Recreate encoder model\n",
    "    encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding(encoder_inputs))\n",
    "    encoder_model = Model(encoder_inputs, [state_h, state_c])\n",
    "    \n",
    "    # Decoder setup\n",
    "    decoder_inputs = Input(shape=(None,))\n",
    "    decoder_embedding = trained_model.get_layer('embedding_1')\n",
    "    decoder_lstm = trained_model.get_layer('lstm_1')\n",
    "    decoder_dense = trained_model.get_layer('dense')\n",
    "    \n",
    "    decoder_state_input_h = Input(shape=(units,))\n",
    "    decoder_state_input_c = Input(shape=(units,))\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    \n",
    "    decoder_outputs = decoder_embedding(decoder_inputs)\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "        decoder_outputs, initial_state=decoder_states_inputs\n",
    "    )\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    \n",
    "    decoder_model = Model(\n",
    "        [decoder_inputs] + decoder_states_inputs,\n",
    "        [decoder_outputs, state_h, state_c]\n",
    "    )\n",
    "    \n",
    "    return encoder_model, decoder_model\n",
    "\n",
    "def generate_command(input_query, max_length=50):\n",
    "    print(\"Processing input...\")\n",
    "    # Input preprocessing\n",
    "    input_seq = input_tokeniser.texts_to_sequences([input_query])\n",
    "    input_seq = pad_sequences(input_seq, maxlen=input_sequences.shape[1], padding='post')\n",
    "    \n",
    "    print(\"Getting encoder predictions...\")\n",
    "    # Get initial states from encoder (reduce verbosity)\n",
    "    states_value = encoder_model.predict(input_seq, verbose=0)\n",
    "    \n",
    "    # Initialize target sequence\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = output_tokeniser.word_index.get('<start>', 1)\n",
    "    \n",
    "    decoded_sentence = []\n",
    "    \n",
    "    print(\"Generating command...\")\n",
    "    for i in range(max_length):\n",
    "        # Reduce prediction verbosity\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value, \n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_word = output_tokeniser.index_word.get(sampled_token_index, '<unk>')\n",
    "        \n",
    "        if sampled_word == '<end>':\n",
    "            break\n",
    "            \n",
    "        if sampled_word not in SPECIAL_TOKENS:\n",
    "            decoded_sentence.append(sampled_word)\n",
    "        \n",
    "        # Update for next iteration\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "        states_value = [h, c]\n",
    "        \n",
    "        if i % 10 == 0:  # Print progress every 10 tokens\n",
    "            print(f\"Generated {i} tokens...\")\n",
    "    \n",
    "    print(\"Command generation complete!\")\n",
    "    return ' '.join(decoded_sentence)\n",
    "\n",
    "# Test with timeout warning\n",
    "import time\n",
    "\n",
    "print(\"Starting command generation...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Set a reasonable timeout (e.g., 30 seconds)\n",
    "TIMEOUT = 30\n",
    "\n",
    "try:\n",
    "    input_query = \"(BSD specific) Display process information twice, waiting one second between each, filtering out the header line.\"\n",
    "    generated_command = generate_command(input_query)\n",
    "    print(f\"\\nGenerated Command: {generated_command}\")\n",
    "    print(f\"\\nTotal time taken: {time.time() - start_time:.2f} seconds\")\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {str(e)}\")\n",
    "finally:\n",
    "    if time.time() - start_time > TIMEOUT:\n",
    "        print(\"\\nWarning: Command generation took longer than expected!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-26 15:32:29.088169: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 686ms/step - accuracy: 0.8399 - loss: 2.4754 - val_accuracy: 0.9007 - val_loss: 0.6449\n",
      "Epoch 2/20\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 689ms/step - accuracy: 0.8901 - loss: 0.7055 - val_accuracy: 0.9078 - val_loss: 0.6096\n",
      "Epoch 3/20\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 699ms/step - accuracy: 0.8929 - loss: 0.6758 - val_accuracy: 0.9094 - val_loss: 0.5837\n",
      "Epoch 4/20\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 703ms/step - accuracy: 0.8952 - loss: 0.6426 - val_accuracy: 0.9107 - val_loss: 0.5629\n",
      "Epoch 5/20\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 695ms/step - accuracy: 0.8973 - loss: 0.6148 - val_accuracy: 0.9113 - val_loss: 0.5485\n",
      "Epoch 6/20\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 707ms/step - accuracy: 0.8998 - loss: 0.5898 - val_accuracy: 0.9126 - val_loss: 0.5373\n",
      "Epoch 7/20\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 704ms/step - accuracy: 0.9011 - loss: 0.5720 - val_accuracy: 0.9145 - val_loss: 0.5264\n",
      "Epoch 8/20\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 710ms/step - accuracy: 0.9027 - loss: 0.5563 - val_accuracy: 0.9154 - val_loss: 0.5180\n",
      "Epoch 9/20\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 698ms/step - accuracy: 0.9054 - loss: 0.5327 - val_accuracy: 0.9167 - val_loss: 0.5091\n",
      "Epoch 10/20\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 693ms/step - accuracy: 0.9057 - loss: 0.5212 - val_accuracy: 0.9167 - val_loss: 0.5019\n",
      "Epoch 11/20\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 686ms/step - accuracy: 0.9079 - loss: 0.4993 - val_accuracy: 0.9184 - val_loss: 0.4901\n",
      "Epoch 12/20\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 684ms/step - accuracy: 0.9097 - loss: 0.4799 - val_accuracy: 0.9186 - val_loss: 0.4845\n",
      "Epoch 13/20\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 685ms/step - accuracy: 0.9102 - loss: 0.4689 - val_accuracy: 0.9189 - val_loss: 0.4776\n",
      "Epoch 14/20\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 687ms/step - accuracy: 0.9123 - loss: 0.4504 - val_accuracy: 0.9194 - val_loss: 0.4752\n",
      "Epoch 15/20\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 683ms/step - accuracy: 0.9139 - loss: 0.4346 - val_accuracy: 0.9198 - val_loss: 0.4707\n",
      "Epoch 16/20\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 685ms/step - accuracy: 0.9156 - loss: 0.4201 - val_accuracy: 0.9205 - val_loss: 0.4681\n",
      "Epoch 17/20\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 691ms/step - accuracy: 0.9176 - loss: 0.4054 - val_accuracy: 0.9204 - val_loss: 0.4667\n",
      "Epoch 18/20\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 645ms/step - accuracy: 0.9200 - loss: 0.3880 - val_accuracy: 0.9212 - val_loss: 0.4639\n",
      "Epoch 19/20\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 608ms/step - accuracy: 0.9209 - loss: 0.3769 - val_accuracy: 0.9217 - val_loss: 0.4604\n",
      "Epoch 20/20\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 608ms/step - accuracy: 0.9243 - loss: 0.3579 - val_accuracy: 0.9229 - val_loss: 0.4566\n"
     ]
    }
   ],
   "source": [
    "# Create and train the model\n",
    "model = create_training_model()\n",
    "history = model.fit(\n",
    "    [input_sequences, output_sequences[:, :-1]],\n",
    "    output_sequences[:, 1:],\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 600ms/step - accuracy: 0.8398 - loss: 2.4929 - val_accuracy: 0.9007 - val_loss: 0.6424\n",
      "Epoch 2/20\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 600ms/step - accuracy: 0.8895 - loss: 0.7079 - val_accuracy: 0.9068 - val_loss: 0.6106\n",
      "Epoch 3/20\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 598ms/step - accuracy: 0.8937 - loss: 0.6705 - val_accuracy: 0.9095 - val_loss: 0.5835\n",
      "Epoch 4/20\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 599ms/step - accuracy: 0.8944 - loss: 0.6480 - val_accuracy: 0.9105 - val_loss: 0.5623\n",
      "Epoch 5/20\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 601ms/step - accuracy: 0.8974 - loss: 0.6133 - val_accuracy: 0.9118 - val_loss: 0.5443\n",
      "Epoch 6/20\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 600ms/step - accuracy: 0.8981 - loss: 0.6005 - val_accuracy: 0.9127 - val_loss: 0.5330\n",
      "Epoch 7/20\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 601ms/step - accuracy: 0.9014 - loss: 0.5698 - val_accuracy: 0.9140 - val_loss: 0.5233\n",
      "Epoch 8/20\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 601ms/step - accuracy: 0.9036 - loss: 0.5514 - val_accuracy: 0.9159 - val_loss: 0.5134\n",
      "Epoch 9/20\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 600ms/step - accuracy: 0.9047 - loss: 0.5366 - val_accuracy: 0.9166 - val_loss: 0.5049\n",
      "Epoch 10/20\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 600ms/step - accuracy: 0.9070 - loss: 0.5146 - val_accuracy: 0.9180 - val_loss: 0.4964\n",
      "Epoch 11/20\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 600ms/step - accuracy: 0.9085 - loss: 0.4980 - val_accuracy: 0.9180 - val_loss: 0.4925\n",
      "Epoch 12/20\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 599ms/step - accuracy: 0.9102 - loss: 0.4820 - val_accuracy: 0.9193 - val_loss: 0.4866\n",
      "Epoch 13/20\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 604ms/step - accuracy: 0.9122 - loss: 0.4631 - val_accuracy: 0.9199 - val_loss: 0.4786\n",
      "Epoch 14/20\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 603ms/step - accuracy: 0.9128 - loss: 0.4529 - val_accuracy: 0.9198 - val_loss: 0.4767\n",
      "Epoch 15/20\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 603ms/step - accuracy: 0.9165 - loss: 0.4278 - val_accuracy: 0.9207 - val_loss: 0.4731\n",
      "Epoch 16/20\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 602ms/step - accuracy: 0.9162 - loss: 0.4240 - val_accuracy: 0.9207 - val_loss: 0.4731\n",
      "Epoch 17/20\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 603ms/step - accuracy: 0.9177 - loss: 0.4111 - val_accuracy: 0.9212 - val_loss: 0.4698\n",
      "Epoch 18/20\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 603ms/step - accuracy: 0.9197 - loss: 0.3957 - val_accuracy: 0.9212 - val_loss: 0.4693\n",
      "Epoch 19/20\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 603ms/step - accuracy: 0.9206 - loss: 0.3840 - val_accuracy: 0.9223 - val_loss: 0.4679\n",
      "Epoch 20/20\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 604ms/step - accuracy: 0.9228 - loss: 0.3716 - val_accuracy: 0.9220 - val_loss: 0.4723\n"
     ]
    }
   ],
   "source": [
    "# Create and train the model\n",
    "model = create_training_model()\n",
    "history = model.fit(\n",
    "    [input_sequences, output_sequences[:, :-1]],\n",
    "    output_sequences[:, 1:],\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create inference models\n",
    "encoder_model, decoder_model = create_inference_models(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "input_query = \"(BSD specific) Display process information twice, waiting one second between each, filtering out the header line.\"\n",
    "generated_command = generate_command(input_query)\n",
    "print(f\"Generated Command: {generated_command}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save('command_generator_model.keras')\n",
    "\n",
    "# Save tokenizers\n",
    "with open('input_tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(input_tokeniser, f)\n",
    "with open('output_tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(output_tokeniser, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dhienv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
